{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13731522,"sourceType":"datasetVersion","datasetId":8736564},{"sourceId":13762542,"sourceType":"datasetVersion","datasetId":8758204},{"sourceId":648502,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":489178,"modelId":504596}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CELL 1\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\npip install transformers==4.36.0 datasets scikit-learn matplotlib seaborn tqdm\n","metadata":{"id":"jsXIZ41dLUF0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 2: Setup and GPU Check (Kaggle Optimized)\n\nimport os\nimport warnings\nimport random\nimport torch\nimport numpy as np\n# Kaggle-specific: Suppress all warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\nos.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\nwarnings.filterwarnings('ignore')\n\n# SET RANDOM SEED FOR REPRODUCIBILITY\ndef set_seed(seed=42):\n    \"\"\"Set all random seeds for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    print(f\"✓ Random seed set to {seed}\")\n\n# Set seed immediately\nset_seed(42)\n# Verify Kaggle GPU\nimport torch\nprint(\"=\"*60)\nprint(\"KAGGLE GPU CHECK\")\nprint(\"=\"*60)\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nelse:\n    print(\" WARNING: No GPU detected!\")\n    print(\" Go to Settings → Accelerator → GPU (P100 or T4)\")\n\n# Load tokenizer\nfrom transformers import XLMRobertaTokenizer\nprint(\"\\nLoading XLM-RoBERTa tokenizer...\")\ntokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\nprint(\"✓ Tokenizer loaded successfully!\")\nprint(\"✓ All warnings suppressed!\")\nprint(\"=\"*60)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"krrwzUgWAAY_","outputId":"5188b8b5-c435-4eaf-957f-84518da0190a","execution":{"iopub.status.busy":"2025-11-17T13:50:40.944717Z","iopub.execute_input":"2025-11-17T13:50:40.945449Z","iopub.status.idle":"2025-11-17T13:50:43.646738Z","shell.execute_reply.started":"2025-11-17T13:50:40.945414Z","shell.execute_reply":"2025-11-17T13:50:43.645865Z"}},"outputs":[{"name":"stdout","text":"✓ Random seed set to 42\n============================================================\nKAGGLE GPU CHECK\n============================================================\nCUDA available: True\nGPU: Tesla T4\nGPU Memory: 14.7 GB\n\nLoading XLM-RoBERTa tokenizer...\n✓ Tokenizer loaded successfully!\n✓ All warnings suppressed!\n============================================================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# CELL 2: Import Everything\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport os\nimport transformers\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport random\nfrom transformers import AutoTokenizer, XLMRobertaModel, get_linear_schedule_with_warmup\nfrom torch.optim import AdamW  # ← Import from torch.optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(f\"✓ PyTorch: {torch.__version__}\")\nprint(f\"✓ Transformers: {transformers.__version__}\")\nprint(f\"✓ CUDA: {torch.cuda.is_available()}\")\nprint(\"✓ ALL READY!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:50:46.803090Z","iopub.execute_input":"2025-11-17T13:50:46.803426Z","iopub.status.idle":"2025-11-17T13:50:52.806425Z","shell.execute_reply.started":"2025-11-17T13:50:46.803403Z","shell.execute_reply":"2025-11-17T13:50:52.805590Z"},"id":"UltMN16_AAZM","outputId":"f150393b-823e-4e3c-f444-0b85e0804d62"},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763387447.550940     178 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763387447.556232     178 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"✓ PyTorch: 2.6.0+cu124\n✓ Transformers: 4.53.3\n✓ CUDA: True\n✓ ALL READY!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ====================\n# CELL 4: Load and Augment Data (Optimized for Your Distribution)\n# ====================\nimport random\n\nset_seed(42)\n\nprint(\"=\"*70)\nprint(\"LOADING AND AUGMENTING MULTILINGUAL DATA\")\nprint(\"=\"*70)\n\nprint(\"\\n Loading training data...\")\neng_train = pd.read_csv('/kaggle/input/dataset/eng_train.csv', encoding='utf-8')\nspa_train = pd.read_csv('/kaggle/input/dataset/spa_train.csv', encoding='utf-8')\ndeu_train = pd.read_csv('/kaggle/input/dataset/deu_train.csv', encoding='utf-8')\n\nprint(f\"  English: {len(eng_train)} samples (Ratio: 1.667:1 )\")\nprint(f\"  Spanish: {len(spa_train)} samples (Ratio: 0.992:1 )\")\nprint(f\"  German: {len(deu_train)} samples (Ratio: 1.103:1 )\")\n\n# ============================================\n# AUGMENT ENGLISH POLARIZED DATA (HEAVY)\n# ============================================\nprint(\"\\n Augmenting English polarized data (TARGET: ~530 samples)...\")\nprint(\"   Reason: English has severe imbalance (62.5% vs 37.5%)\")\n\npolarized_eng = eng_train[eng_train['polarization'] == 1].copy()\nprint(f\"   Original English polarized: {len(polarized_eng)} samples\")\nprint(f\"   Target after augmentation: ~1,335 samples\")\n\naugmented_eng = []\ntarget_augmentation = 534  # To balance English\n\n# More aggressive augmentation techniques\nfor idx, row in polarized_eng.iterrows():\n    text = row['text']\n    words = text.split()\n    \n    if len(words) < 3:\n        continue\n    \n    # Technique 1: Add emphasis words (40% chance)\n    if random.random() < 0.4 and len(augmented_eng) < target_augmentation:\n        emphasis_words = ['really', 'very', 'extremely', 'absolutely', 'totally', \n                          'completely', 'clearly', 'obviously', 'definitely']\n        emphasis = random.choice(emphasis_words)\n        insert_pos = random.randint(1, min(4, len(words)-1))\n        words_copy = words.copy()\n        words_copy.insert(insert_pos, emphasis)\n        augmented_eng.append({\n            'text': ' '.join(words_copy),\n            'polarization': 1\n        })\n    \n    # Technique 2: Synonym replacement for common words (30% chance)\n    if random.random() < 0.3 and len(augmented_eng) < target_augmentation:\n        synonyms = {\n            'bad': ['terrible', 'awful', 'horrible'],\n            'good': ['great', 'excellent', 'wonderful'],\n            'hate': ['despise', 'detest', 'loathe'],\n            'love': ['adore', 'cherish', 'treasure'],\n            'stupid': ['foolish', 'idiotic', 'moronic'],\n            'smart': ['intelligent', 'clever', 'bright']\n        }\n        words_copy = words.copy()\n        for i, word in enumerate(words_copy):\n            word_lower = word.lower()\n            if word_lower in synonyms and random.random() < 0.5:\n                words_copy[i] = random.choice(synonyms[word_lower])\n                break\n        \n        augmented_text = ' '.join(words_copy)\n        if augmented_text != text:  \n            augmented_eng.append({\n                'text': augmented_text,\n                'polarization': 1\n            })\n    \n    # Technique 3: Word swap (25% chance)\n    if random.random() < 0.25 and len(words) > 4 and len(augmented_eng) < target_augmentation:\n        words_copy = words.copy()\n        swap_pos = random.randint(1, len(words_copy)-3)\n        \n        skip_words = ['the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', \n                      'of', 'to', 'in', 'for', 'on', 'at', 'by', 'with']\n        \n        if (words_copy[swap_pos].lower() not in skip_words and \n            words_copy[swap_pos+1].lower() not in skip_words):\n            words_copy[swap_pos], words_copy[swap_pos+1] = words_copy[swap_pos+1], words_copy[swap_pos]\n            augmented_eng.append({\n                'text': ' '.join(words_copy),\n                'polarization': 1\n            })\n    \n    # Technique 4: Duplicate with minor variation (20% chance)\n    if random.random() < 0.2 and len(augmented_eng) < target_augmentation:\n        words_copy = words.copy()\n        # Add a filler word at the beginning\n        fillers = ['Well,', 'So,', 'Actually,', 'Honestly,', 'Basically,']\n        words_copy.insert(0, random.choice(fillers))\n        augmented_eng.append({\n            'text': ' '.join(words_copy),\n            'polarization': 1\n        })\n    \n    # Stop if we've reached our target\n    if len(augmented_eng) >= target_augmentation:\n        break\n\n# Add augmented data to English training set\nif augmented_eng:\n    augmented_df = pd.DataFrame(augmented_eng)\n    eng_train = pd.concat([eng_train, augmented_df], ignore_index=True)\n    \n    # Check new balance\n    eng_pol_counts = eng_train['polarization'].value_counts().sort_index()\n    new_eng_ratio = eng_pol_counts[0] / eng_pol_counts[1]\n    \n    print(f\"\\n    Added {len(augmented_eng)} augmented English samples\")\n    print(f\"   New English distribution:\")\n    print(f\"      Non-Polarized: {eng_pol_counts[0]}\")\n    print(f\"      Polarized: {eng_pol_counts[1]}\")\n    print(f\"      New ratio: {new_eng_ratio:.3f}:1 {'✅' if new_eng_ratio < 1.1 else '⚠️'}\")\nelse:\n    print(f\"   No augmentation performed\")\n\n# ============================================\n# OPTIONAL: LIGHT GERMAN AUGMENTATION\n# ============================================\nprint(\"\\n German augmentation (light - optional)...\")\nprint(\"   German is already well-balanced (1.103:1), adding just ~50 samples\")\n\npolarized_deu = deu_train[deu_train['polarization'] == 1].copy()\naugmented_deu = []\ntarget_deu = 50\n\nfor idx, row in polarized_deu.iterrows():\n    text = row['text']\n    words = text.split()\n    \n    if len(words) > 4 and random.random() < 0.3 and len(augmented_deu) < target_deu:\n        # Add German emphasis words\n        emphasis = random.choice(['sehr', 'wirklich', 'extrem', 'absolut', 'total', 'völlig'])\n        insert_pos = random.randint(1, min(3, len(words)-1))\n        words_copy = words.copy()\n        words_copy.insert(insert_pos, emphasis)\n        augmented_deu.append({\n            'text': ' '.join(words_copy),\n            'polarization': 1\n        })\n    \n    if len(augmented_deu) >= target_deu:\n        break\n\nif augmented_deu:\n    deu_train = pd.concat([deu_train, pd.DataFrame(augmented_deu)], ignore_index=True)\n    print(f\"    Added {len(augmented_deu)} augmented German samples\")\nelse:\n    print(f\"    Skipped German augmentation (already balanced)\")\n\nprint(\"\\n Spanish augmentation: SKIPPED\")\nprint(\"   Spanish is perfectly balanced (0.992:1) - no augmentation needed!\")\n\n# ============================================\n# ADD LANGUAGE LABELS\n# ============================================\neng_train['language'] = 'en'\nspa_train['language'] = 'es'\ndeu_train['language'] = 'de'\n\nprint(\"\\n Combining training data...\")\ntrain_df = pd.concat([eng_train, spa_train, deu_train], ignore_index=True)\ntrain_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\" FINAL TRAINING DATA STATISTICS (AFTER AUGMENTATION)\")\nprint(\"=\"*70)\n\ntotal_samples = len(train_df)\npol_counts = train_df['polarization'].value_counts().sort_index()\nfinal_ratio = pol_counts[0] / pol_counts[1]\n\nprint(f\"\\n  Total samples: {total_samples:,}\")\nprint(f\"  Non-Polarized (0): {pol_counts[0]:,} ({pol_counts[0]/total_samples*100:.1f}%)\")\nprint(f\"  Polarized (1): {pol_counts[1]:,} ({pol_counts[1]/total_samples*100:.1f}%)\")\nprint(f\"  Final ratio: {final_ratio:.3f}:1\")\n\nif final_ratio < 1.05:\n    balance_status = \" EXCELLENT - Nearly Perfect!\"\n    weight_recommendation = \"NO class weights needed\"\nelif final_ratio < 1.10:\n    balance_status = \" VERY GOOD - Well Balanced\"\n    weight_recommendation = \"Use NO weights OR very light [1.0, 1.03]\"\nelif final_ratio < 1.15:\n    balance_status = \" GOOD - Acceptable Balance\"\n    weight_recommendation = \"Use light weights [1.0, 1.05]\"\nelse:\n    balance_status = \" MODERATE - Some Imbalance Remains\"\n    weight_recommendation = \"Use moderate weights [1.0, 1.10]\"\n\nprint(f\"\\n  Balance Status: {balance_status}\")\nprint(f\"   Recommendation: {weight_recommendation}\")\n\n# Language breakdown\nprint(f\"\\n  By Language:\")\nfor lang, lang_name in [('en', 'English'), ('es', 'Spanish'), ('de', 'German')]:\n    lang_df = train_df[train_df['language'] == lang]\n    lang_pol = lang_df['polarization'].value_counts().sort_index()\n    lang_ratio = lang_pol[0] / lang_pol[1] if lang_pol[1] > 0 else 0\n    print(f\"    {lang_name}: {len(lang_df):,} samples (Ratio: {lang_ratio:.3f}:1)\")\n\nprint(\"\\n Loading test data...\")\neng_test = pd.read_csv('/kaggle/input/dataset/eng_test.csv', encoding='utf-8')\nspa_test = pd.read_csv('/kaggle/input/dataset/spa_test.csv', encoding='utf-8')\ndeu_test = pd.read_csv('/kaggle/input/dataset/deu_test.csv', encoding='utf-8')\n\neng_test['language'] = 'en'\nspa_test['language'] = 'es'\ndeu_test['language'] = 'de'\n\ntest_df = pd.concat([eng_test, spa_test, deu_test], ignore_index=True)\n\nprint(f\"  Total test samples: {len(test_df):,}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\" DATA LOADING AND AUGMENTATION COMPLETE!\")\nprint(\"=\"*70)\n\nprint(f\"\\n Augmentation Summary:\")\nprint(f\"  • English: +{len(augmented_eng)} samples (balanced severe imbalance)\")\nprint(f\"  • Spanish: +0 samples (already perfect)\")\nprint(f\"  • German: +{len(augmented_deu) if augmented_deu else 0} samples (light touch-up)\")\nprint(f\"  • Total added: {len(augmented_eng) + (len(augmented_deu) if augmented_deu else 0)} samples\")\n\nprint(f\"\\n Impact:\")\nprint(f\"  • Before: 1.194:1 (54.4% vs 45.6%) \")\nprint(f\"  • After: {final_ratio:.3f}:1 ({pol_counts[0]/total_samples*100:.1f}% vs {pol_counts[1]/total_samples*100:.1f}%) {balance_status.split()[0]}\")\nprint(f\"  • Improvement: {((1.194 - final_ratio) / 1.194 * 100):.1f}% better balance!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================\n# CELL 5: Data Inspection & Preprocessing\n# ====================\n\nTEXT_COLUMN = 'text'           \nLABEL_COLUMN = 'polarization' \n\nprint(\"Dataset Information:\")\nprint(f\"Text column: '{TEXT_COLUMN}'\")\nprint(f\"Label column: '{LABEL_COLUMN}'\")\nprint(f\"Task: Binary polarized Speech Detection (Polarization)\")\nprint(f\"  0 = Non-polarized speech\")\nprint(f\"  1 = polarized speech / Negative polarization\")\n\n# Check for missing values\nprint(f\"\\nMissing values in training data:\")\nprint(train_df[[TEXT_COLUMN, LABEL_COLUMN]].isnull().sum())\n\n# Check label distribution\nprint(f\"\\nLabel distribution in training data:\")\nlabel_counts = train_df[LABEL_COLUMN].value_counts().sort_index()\nprint(label_counts)\nprint(f\"\\nClass breakdown:\")\nprint(f\"  Class 0 (Non-polarized): {label_counts.get(0, 0)} samples ({label_counts.get(0, 0)/len(train_df)*100:.1f}%)\")\nprint(f\"  Class 1 (polarized speech): {label_counts.get(1, 0)} samples ({label_counts.get(1, 0)/len(train_df)*100:.1f}%)\")\n\n# Clean data\ndef clean_data(df, text_col, label_col):\n    \"\"\"Basic data cleaning.\"\"\"\n    initial_size = len(df)\n\n    # Remove missing values\n    df = df.dropna(subset=[text_col, label_col])\n\n    # Remove empty strings\n    df = df[df[text_col].str.strip() != '']\n\n    # Remove duplicates\n    df = df.drop_duplicates(subset=[text_col])\n\n    # Reset index\n    df = df.reset_index(drop=True)\n\n    print(f\"Cleaned: {initial_size} → {len(df)} samples\")\n    return df\n\ntrain_df = clean_data(train_df, TEXT_COLUMN, LABEL_COLUMN)\ntest_df = clean_data(test_df, TEXT_COLUMN, LABEL_COLUMN)\n\n# Create validation set from training data (10%)\nfrom sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(\n    train_df,\n    test_size=0.1,\n    random_state=42,\n    stratify=train_df[LABEL_COLUMN]\n)\n\nprint(f\"\\nFinal data splits:\")\nprint(f\"  Train: {len(train_df)} samples\")\nprint(f\"  Val:   {len(val_df)} samples\")\nprint(f\"  Test:  {len(test_df)} samples\")\n\n# Get number of classes\nnum_classes = len(train_df[LABEL_COLUMN].unique())\nprint(f\"\\nNumber of classes: {num_classes}\")\nprint(f\"Classes: {sorted(train_df[LABEL_COLUMN].unique())}\")\n","metadata":{"trusted":true,"id":"m0MGIs9lAAZX","outputId":"b01b2a29-3f7e-40e6-bcf2-fd58c816549c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================\n# CELL 6: Create PyTorch Dataset\n# ====================\n\nclass MultilingualTextDataset(Dataset):\n    \"\"\"Dataset for XLM-RoBERTa multilingual text classification.\"\"\"\n\n    def __init__(self, texts, labels, tokenizer, max_length=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n\n        # Tokenize\n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\nprint(\" Dataset class defined!\")\n","metadata":{"trusted":true,"id":"qBNjM8nyAAZZ","outputId":"231d8de1-6e2f-40f7-b783-68b62c04d56b"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================\n# CELL 7: Model Definition with Loss Functions (OPTIMIZED FOR BALANCED DATA)\n# ====================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import XLMRobertaModel\n\n# ============================================\n# FOCAL LOSS (OPTIMIZED FOR BALANCED POLARIZATION DATA)\n# ============================================\nclass ClassBalancedFocalLoss(nn.Module):\n    \"\"\"\n    Focal loss with class-specific weights\n    Optimized for balanced polarization detection (54% vs 46%)\n    \"\"\"\n    def __init__(self, alpha=None, gamma=None, class_weights=None, label_smoothing=0.1):\n        super().__init__()\n        # Default values optimized for balanced data\n        self.alpha = alpha if alpha is not None else 0.5  # Equal weight for balanced classes\n        self.gamma = gamma if gamma is not None else 1.5  # Moderate difficulty focus\n        self.class_weights = class_weights\n        self.label_smoothing = label_smoothing\n\n    def forward(self, inputs, targets):\n        # Apply label smoothing\n        if self.label_smoothing > 0:\n            n_classes = inputs.size(-1)\n            targets_one_hot = F.one_hot(targets, n_classes).float()\n            targets_smoothed = targets_one_hot * (1 - self.label_smoothing) + self.label_smoothing / n_classes\n            ce_loss = -(targets_smoothed * F.log_softmax(inputs, dim=-1)).sum(dim=-1)\n        else:\n            ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        \n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n\n        # Apply class weights if provided\n        if self.class_weights is not None:\n            weights = self.class_weights[targets].to(inputs.device)\n            focal_loss = focal_loss * weights\n\n        return focal_loss.mean()\n\n\n# ============================================\n# MIXUP HELPERS\n# ============================================\ndef mixup_data(x, y, alpha=0.2):\n    \"\"\"\n    Apply mixup augmentation to embeddings\n    Lighter alpha (0.2) for balanced data\n    \"\"\"\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1.0\n\n    batch_size = x.size(0)\n    index = torch.randperm(batch_size).to(x.device)\n\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n\n    return mixed_x, y_a, y_b, lam\n\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    \"\"\"Calculate mixup loss.\"\"\"\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\n\n# ============================================\n# MODEL DEFINITION (OPTIMIZED FOR POLARIZATION DETECTION)\n# ============================================\nclass MacroF1OptimizedModel(nn.Module):\n    \"\"\"\n    XLM-RoBERTa optimized for Macro F1-Score on polarization detection\n    \n    Features:\n    - Multi-sample dropout ensemble (reduces overfitting)\n    - Attention pooling (better sequence representation)\n    - Class-balanced focal loss (handles slight imbalance: 54% vs 46%)\n    - Optimized for detecting \"us vs them\" polarization patterns\n    \"\"\"\n\n    def __init__(self, num_classes=2, model_name='xlm-roberta-large',\n                 dropout=0.2, num_dropouts=5, class_weights=None, \n                 focal_alpha=0.5, focal_gamma=1.5):\n        super().__init__()\n\n        self.roberta = XLMRobertaModel.from_pretrained(model_name)\n        hidden_size = self.roberta.config.hidden_size\n\n        # Attention pooling for better context understanding\n        self.attention = nn.Linear(hidden_size, 1)\n\n        # Multi-sample dropout for ensemble effect\n        self.dropouts = nn.ModuleList([\n            nn.Dropout(dropout) for _ in range(num_dropouts)\n        ])\n\n        # Classification head\n        self.classifier = nn.Linear(hidden_size, num_classes)\n\n        # Loss function optimized for balanced data\n        self.loss_fn = ClassBalancedFocalLoss(\n            alpha=focal_alpha,\n            gamma=focal_gamma,\n            class_weights=class_weights,\n            label_smoothing=0.1  # Slight smoothing for better generalization\n        )\n\n        # Initialize weights\n        nn.init.normal_(self.classifier.weight, std=0.02)\n        nn.init.zeros_(self.classifier.bias)\n\n    def attention_pooling(self, hidden_states, attention_mask):\n        \"\"\"\n        Attention-based pooling over sequence\n        Better than [CLS] token for capturing polarization markers throughout text\n        \"\"\"\n        attention_weights = self.attention(hidden_states)\n        attention_weights = attention_weights.masked_fill(\n            attention_mask.unsqueeze(-1) == 0, -1e9\n        )\n        attention_weights = F.softmax(attention_weights, dim=1)\n        pooled = torch.sum(hidden_states * attention_weights, dim=1)\n        return pooled\n\n    def forward(self, input_ids, attention_mask, labels=None,\n                embeddings=None, mixup_params=None):\n        # Get RoBERTa outputs\n        if embeddings is not None:\n            outputs = self.roberta(inputs_embeds=embeddings, attention_mask=attention_mask)\n        else:\n            outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n\n        # Attention pooling\n        pooled_output = self.attention_pooling(outputs.last_hidden_state, attention_mask)\n\n        # Multi-sample dropout ensemble (reduces variance)\n        if self.training:\n            logits_list = []\n            for dropout in self.dropouts:\n                dropped = dropout(pooled_output)\n                logits_list.append(self.classifier(dropped))\n            logits = torch.stack(logits_list).mean(dim=0)\n        else:\n            pooled_output = self.dropouts[0](pooled_output)\n            logits = self.classifier(pooled_output)\n\n        # Calculate loss\n        loss = None\n        if labels is not None:\n            if mixup_params is not None:\n                y_a, y_b, lam = mixup_params\n                loss = mixup_criterion(self.loss_fn, logits, y_a, y_b, lam)\n            else:\n                loss = self.loss_fn(logits, labels)\n\n        return {\n            'loss': loss,\n            'logits': logits,\n            'pooled_output': pooled_output\n        }\n\n\nprint(\"✓ MacroF1OptimizedModel for Polarization Detection!\")\nprint(\"\\n Model Architecture:\")\nprint(\"   Attention pooling (captures 'us vs them' throughout text)\")\nprint(\"   Multi-sample dropout (n=5, ensemble effect)\")\nprint(\"   Balanced focal loss (α=0.5, γ=1.5)\")\nprint(\"   Label smoothing (0.1, better generalization)\")\n\n\n","metadata":{"trusted":true,"id":"Y6uXMMnwAAZa","outputId":"ee4ac8f2-e641-4fa6-abb1-843bcc7f7b96","execution":{"iopub.status.busy":"2025-11-17T13:51:02.838741Z","iopub.execute_input":"2025-11-17T13:51:02.840036Z","iopub.status.idle":"2025-11-17T13:51:02.864516Z","shell.execute_reply.started":"2025-11-17T13:51:02.839998Z","shell.execute_reply":"2025-11-17T13:51:02.863506Z"}},"outputs":[{"name":"stdout","text":"✓ MacroF1OptimizedModel for Polarization Detection!\n\n Model Architecture:\n   Attention pooling (captures 'us vs them' throughout text)\n   Multi-sample dropout (n=5, ensemble effect)\n   Balanced focal loss (α=0.5, γ=1.5)\n   Label smoothing (0.1, better generalization)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ====================\n# CELL 8: Configuration and Model Setup \n# ====================\nimport gc\nset_seed(42) \n# Clear GPU memory\ntorch.cuda.empty_cache()\ngc.collect()\n\n# ============================================\n# CONFIGURATION \n# ============================================\nCONFIG = {\n    'model_name': 'xlm-roberta-large',  \n    'max_length': 128,\n    'batch_size': 8,\n    'num_epochs': 20,                  \n    'learning_rate': 1e-6,             \n    'weight_decay': 0.03,\n    'dropout': 0.5,                    \n    'num_dropouts': 5,\n    'gradient_accumulation_steps': 4,\n    'max_grad_norm': 1.0,\n    'warmup_steps': 1000,\n    'use_mixup': True,\n    'mixup_alpha': 0.2,\n    'use_tta': True,\n    'tta_iterations': 5,\n    'focal_alpha': 0.5,\n    'focal_gamma': 2.0,\n}\nMODEL_NAME = 'model_4_large_careful'  \n\nprint(\"OPTIMIZED Configuration for XLM-RoBERTa-Large:\")\nprint(\"=\"*70)\nfor key, value in CONFIG.items():\n    print(f\"  {key}: {value}\")\nprint(\"=\"*70)\n\n\n# ============================================\n# CREATE DATASETS AND DATALOADERS\n# ============================================\nprint(\"\\nCreating datasets...\")\ntokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\ntrain_dataset = MultilingualTextDataset(\n    train_df[TEXT_COLUMN].tolist(),\n    train_df[LABEL_COLUMN].tolist(),\n    tokenizer,\n    CONFIG['max_length']\n)\n\nval_dataset = MultilingualTextDataset(\n    val_df[TEXT_COLUMN].tolist(),\n    val_df[LABEL_COLUMN].tolist(),\n    tokenizer,\n    CONFIG['max_length']\n)\n\ntest_dataset = MultilingualTextDataset(\n    test_df[TEXT_COLUMN].tolist(),\n    test_df[LABEL_COLUMN].tolist(),\n    tokenizer,\n    CONFIG['max_length']\n)\n\nprint(\"✓ Datasets created!\")\n\n# ============================================\n# CREATE DATALOADERS\n# ============================================\nprint(\"\\nCreating dataloaders...\")\n\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=CONFIG['batch_size'], \n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=CONFIG['batch_size'], \n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=CONFIG['batch_size'], \n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\nprint(f\"\\nDataLoaders ready:\")\nprint(f\"  Train batches: {len(train_loader)}\")\nprint(f\"  Val batches: {len(val_loader)}\")\nprint(f\"  Test batches: {len(test_loader)}\")\n\n# ============================================\n# INITIALIZE MODEL\n# ============================================\nprint(\"\\nInitializing device...\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")\n\n# NO CLASS WEIGHTS - data is balanced\nclass_weights = None\nprint(f\"\\n Using NO class weights (data is balanced: 1.016:1)\")\n\n# Initialize model\nprint(\"\\nInitializing XLM-RoBERTa model...\")\nmodel = MacroF1OptimizedModel(\n    num_classes=num_classes,\n    model_name=CONFIG['model_name'],\n    dropout=CONFIG['dropout'],\n    num_dropouts=CONFIG['num_dropouts'],\n    class_weights=class_weights,\n    focal_alpha=CONFIG['focal_alpha'],\n    focal_gamma=CONFIG['focal_gamma']\n)\nmodel = model.to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\" Model initialized!\")\nprint(f\"  Trainable parameters: {trainable_params:,}\")\n\nif torch.cuda.is_available():\n    print(f\"\\nGPU Memory:\")\n    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n    print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n\n","metadata":{"trusted":true,"id":"ncDYHK1pAAZc","outputId":"0439483b-a2f1-40c4-b41c-569090c073e7"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================\n# CELL 9: Training & Evaluation Functions\n# ====================\nfrom sklearn.metrics import f1_score, precision_recall_fscore_support\n\n# ============================================\n# TEST-TIME AUGMENTATION\n# ============================================\ndef predict_with_tta(model, batch, device, n_iterations=5):\n    \"\"\"Test-time augmentation for better predictions.\"\"\"\n    model.eval()\n    all_logits = []\n\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n\n    with torch.no_grad():\n        for _ in range(n_iterations):\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs['logits']\n            all_logits.append(logits)\n\n    avg_logits = torch.stack(all_logits).mean(dim=0)\n    preds = torch.argmax(avg_logits, dim=1)\n    return preds, avg_logits\n\n\n# ============================================\n# THRESHOLD OPTIMIZATION\n# ============================================\ndef optimize_threshold(model, val_loader, device):\n    \"\"\"Find optimal classification threshold for Macro F1.\"\"\"\n    model.eval()\n    all_probs = []\n    all_labels = []\n\n    print(\"\\nOptimizing threshold...\")\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc='Computing probabilities'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].cpu().numpy()\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs['logits']\n            probs = F.softmax(logits, dim=1)[:, 1]\n\n            all_probs.extend(probs.cpu().numpy())\n            all_labels.extend(labels)\n\n    # Try different thresholds\n    best_threshold = 0.5\n    best_macro_f1 = -1.0\n\n    for threshold in np.arange(0.3, 0.7, 0.01):\n        preds = (np.array(all_probs) >= threshold).astype(int)\n        macro_f1 = f1_score(all_labels, preds, average='macro')\n\n        if macro_f1 > best_macro_f1:\n            best_macro_f1 = macro_f1\n            best_threshold = float(threshold)\n\n    print(f\"Optimal Threshold: {best_threshold:.3f}\")\n    print(f\"Macro F1 at optimal threshold: {best_macro_f1:.4f}\")\n\n    return best_threshold\n\n\n# ============================================\n# TRAINING FUNCTION\n# ============================================\ndef train_epoch(model, data_loader, optimizer, scheduler, device):\n    \"\"\"Training loop with mixup and gradient accumulation.\"\"\"\n    model.train()\n    total_loss = 0.0\n    all_preds = []\n    all_labels = []\n\n    optimizer.zero_grad()\n    progress_bar = tqdm(data_loader, desc='Training')\n\n    for batch_idx, batch in enumerate(progress_bar):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        # Apply mixup randomly (50% of batches)\n        if CONFIG['use_mixup'] and np.random.rand() > 0.5:\n            with torch.no_grad():\n                embeddings = model.roberta.embeddings(input_ids=input_ids)\n            mixed_embeddings, y_a, y_b, lam = mixup_data(embeddings, labels, CONFIG['mixup_alpha'])\n\n            outputs = model(\n                input_ids=None,\n                attention_mask=attention_mask,\n                labels=labels,\n                embeddings=mixed_embeddings,\n                mixup_params=(y_a, y_b, lam)\n            )\n        else:\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n\n        loss = outputs['loss']\n        logits = outputs['logits']\n\n        # Scale loss for gradient accumulation\n        loss = loss / CONFIG['gradient_accumulation_steps']\n        loss.backward()\n\n        # Gradient accumulation step\n        if (batch_idx + 1) % CONFIG['gradient_accumulation_steps'] == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n        # Track metrics\n        with torch.no_grad():\n            preds = torch.argmax(logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n        total_loss += (loss.item() * CONFIG['gradient_accumulation_steps'])\n\n        # Update progress bar\n        if len(all_labels) > 0:\n            current_f1 = f1_score(all_labels, all_preds, average='macro')\n            progress_bar.set_postfix({\n                'loss': f\"{(loss.item() * CONFIG['gradient_accumulation_steps']):.4f}\",\n                'macro_f1': f\"{current_f1:.4f}\"\n            })\n\n    avg_loss = total_loss / len(data_loader)\n    epoch_f1 = f1_score(all_labels, all_preds, average='macro')\n\n    return avg_loss, epoch_f1\n\n\n# ============================================\n# EVALUATION FUNCTION\n# ============================================\ndef evaluate(model, data_loader, device, threshold=0.5, use_tta=False):\n    \"\"\"Evaluation with optional TTA and custom threshold.\"\"\"\n    model.eval()\n    total_loss = 0.0\n    all_probs = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(data_loader, desc='Evaluating'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            if use_tta and CONFIG['use_tta']:\n                preds, avg_logits = predict_with_tta(model, batch, device, CONFIG['tta_iterations'])\n                probs = F.softmax(avg_logits, dim=1)\n            else:\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                loss = outputs['loss']\n                logits = outputs['logits']\n                total_loss += loss.item()\n                probs = F.softmax(logits, dim=1)\n\n            all_probs.extend(probs.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    # Apply threshold to positive-class probability\n    all_probs = np.array(all_probs)\n    all_preds = (all_probs[:, 1] >= threshold).astype(int)\n\n    # Calculate metrics\n    avg_loss = total_loss / len(data_loader) if total_loss > 0 else 0.0\n    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n    weighted_f1 = f1_score(all_labels, all_preds, average='weighted')\n\n    per_class_metrics = precision_recall_fscore_support(all_labels, all_preds, average=None)\n\n    return {\n        'loss': avg_loss,\n        'macro_f1': macro_f1,\n        'weighted_f1': weighted_f1,\n        'per_class_f1': per_class_metrics[2],\n        'predictions': all_preds,\n        'labels': all_labels\n    }\n\nprint(\" Training and evaluation functions defined!\")","metadata":{"trusted":true,"id":"pkPTcGEJAAZe","outputId":"88f1b135-56a7-42df-9916-36f65f7fba52"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport torch\n\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(\" GPU memory cleared\")\nprint(f\"Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\nprint(f\"Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================\n# CELL 10: Optimized Training for Balanced Polarization Data\n# ====================\n\nimport gc\nimport os\nfrom torch.optim.lr_scheduler import OneCycleLR\nimport copy\n\nset_seed(42)\n\n# ============================================\n# MEMORY OPTIMIZATION\n# ============================================\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512,expandable_segments:True'\n\ntorch.cuda.empty_cache()\ngc.collect()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\" Device: {device}\")\nif torch.cuda.is_available():\n    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n\nmodel = model.to(device)\n\n# ============================================\n# LAYERWISE LEARNING RATE (OPTIMIZED FOR POLARIZATION)\n# ============================================\ndef get_optimizer_grouped_parameters(model, learning_rate, weight_decay):\n    \"\"\"\n    Create parameter groups with layerwise learning rates\n    Optimized for polarization detection (needs more semantic understanding)\n    \"\"\"\n    no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n    optimizer_grouped_parameters = []\n    processed_params = set()\n    \n    # 1. Embeddings - 15% of base LR (slightly higher for polarization)\n    embeddings_params_decay = []\n    embeddings_params_no_decay = []\n    \n    for n, p in model.roberta.embeddings.named_parameters():\n        if p.requires_grad:\n            processed_params.add(id(p))\n            if any(nd in n for nd in no_decay):\n                embeddings_params_no_decay.append(p)\n            else:\n                embeddings_params_decay.append(p)\n    \n    if embeddings_params_decay:\n        optimizer_grouped_parameters.append({\n            \"params\": embeddings_params_decay,\n            \"weight_decay\": weight_decay,\n            \"lr\": learning_rate * 0.15  # ← Increased from 0.1 for better word understanding\n        })\n    if embeddings_params_no_decay:\n        optimizer_grouped_parameters.append({\n            \"params\": embeddings_params_no_decay,\n            \"weight_decay\": 0.0,\n            \"lr\": learning_rate * 0.15\n        })\n    \n    # 2. Encoder layers - gradually increasing LR\n    num_layers = model.roberta.config.num_hidden_layers\n    \n    for layer_num in range(num_layers):\n        layer = model.roberta.encoder.layer[layer_num]\n        # Smoother gradient: 15% to 100%\n        layer_lr_multiplier = 0.15 + (0.85 * layer_num / (num_layers - 1))\n        \n        layer_params_decay = []\n        layer_params_no_decay = []\n        \n        for n, p in layer.named_parameters():\n            if p.requires_grad and id(p) not in processed_params:\n                processed_params.add(id(p))\n                if any(nd in n for nd in no_decay):\n                    layer_params_no_decay.append(p)\n                else:\n                    layer_params_decay.append(p)\n        \n        if layer_params_decay:\n            optimizer_grouped_parameters.append({\n                \"params\": layer_params_decay,\n                \"weight_decay\": weight_decay,\n                \"lr\": learning_rate * layer_lr_multiplier\n            })\n        if layer_params_no_decay:\n            optimizer_grouped_parameters.append({\n                \"params\": layer_params_no_decay,\n                \"weight_decay\": 0.0,\n                \"lr\": learning_rate * layer_lr_multiplier\n            })\n    \n    # 3. Task-specific layers (attention, classifier) - 2x base LR\n    task_params_decay = []\n    task_params_no_decay = []\n    \n    for n, p in model.named_parameters():\n        if p.requires_grad and id(p) not in processed_params:\n            if \"classifier\" in n or \"attention\" in n:\n                processed_params.add(id(p))\n                if any(nd in n for nd in no_decay):\n                    task_params_no_decay.append(p)\n                else:\n                    task_params_decay.append(p)\n    \n    if task_params_decay:\n        optimizer_grouped_parameters.append({\n            \"params\": task_params_decay,\n            \"weight_decay\": weight_decay,\n            \"lr\": learning_rate * 2.0  # Fast adaptation to polarization patterns\n        })\n    if task_params_no_decay:\n        optimizer_grouped_parameters.append({\n            \"params\": task_params_no_decay,\n            \"weight_decay\": 0.0,\n            \"lr\": learning_rate * 2.0\n        })\n    \n    return optimizer_grouped_parameters\n\n# ============================================\n# OPTIMIZER WITH LAYERWISE LR\n# ============================================\nBASE_LR = CONFIG['learning_rate']  # 3e-5\noptimizer_params = get_optimizer_grouped_parameters(\n    model, \n    BASE_LR, \n    weight_decay=CONFIG['weight_decay']\n)\n\noptimizer = AdamW(\n    optimizer_params,\n    betas=(0.9, 0.999),\n    eps=1e-8\n)\n\nprint(f\"\\n✓ Layerwise Learning Rates (Optimized for Polarization):\")\nprint(f\"  Embeddings:      {BASE_LR * 0.15:.2e} (15% - word meaning)\")\nprint(f\"  Lower layers:    {BASE_LR * 0.15:.2e} - {BASE_LR * 0.5:.2e} (context)\")\nprint(f\"  Upper layers:    {BASE_LR * 0.5:.2e} - {BASE_LR:.2e} (semantics)\")\nprint(f\"  Classifier:      {BASE_LR * 2.0:.2e} (200% - task-specific)\")\n\n# ============================================\n# ONE-CYCLE LR SCHEDULER\n# ============================================\ntotal_steps = len(train_loader) * CONFIG['num_epochs'] // CONFIG['gradient_accumulation_steps']\n\nscheduler = OneCycleLR(\n    optimizer,\n    max_lr=[param_group['lr'] for param_group in optimizer.param_groups],\n    total_steps=total_steps,\n    pct_start=0.1,  # 10% warmup\n    anneal_strategy='cos',\n    div_factor=25.0,\n    final_div_factor=10000.0\n)\n\nprint(f\"\\n✓ OneCycleLR Scheduler:\")\nprint(f\"  Total steps: {total_steps}\")\nprint(f\"  Warmup steps: {int(total_steps * 0.1)} (10%)\")\nprint(f\"  Annealing: Cosine\")\n\n# Training history\nhistory = {\n    'train_loss': [],\n    'train_macro_f1': [],\n    'val_loss': [],\n    'val_macro_f1': [],\n    'val_weighted_f1': []\n}\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"POLARIZATION DETECTION TRAINING \")\nprint(\"=\"*70)\nprint(f\"\\n  Training Configuration:\")\nprint(f\"  Epochs: {CONFIG['num_epochs']}\")\nprint(f\"  Batch size: {CONFIG['batch_size']} (effective: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']})\")\nprint(f\"  Base LR: {BASE_LR:.2e}\")\nprint(f\"  Warmup: 10% of training\")\nprint(f\"  Device: {device}\")\nprint(f\"\\n⏱️  Estimated Time: ~45-60 minutes\")\nprint(\"=\"*70)\nprint(\"\\n\")\n\nbest_val_macro_f1 = 0\nbest_epoch = 0\npatience = 0\nearly_stopping_patience = 3\n\nfor epoch in range(CONFIG['num_epochs']):\n    print(f\"\\nEpoch {epoch + 1}/{CONFIG['num_epochs']}\")\n    print(\"-\" * 70)\n    \n    try:\n        # Train\n        train_loss, train_macro_f1 = train_epoch(\n            model, train_loader, optimizer, scheduler, device\n        )\n        \n        print(f\"Train Loss: {train_loss:.4f} | Train Macro-F1: {train_macro_f1:.4f}\")\n\n        # Validate (no TTA during training for speed)\n        val_results = evaluate(model, val_loader, device, threshold=0.5, use_tta=False)\n        print(f\"Val Loss: {val_results['loss']:.4f} | Val Macro-F1: {val_results['macro_f1']:.4f} | Val Weighted-F1: {val_results['weighted_f1']:.4f}\")\n        \n        # Calculate train-val gap (overfitting indicator)\n        f1_gap = train_macro_f1 - val_results['macro_f1']\n        gap_status = \" OVERFITTING\" if f1_gap > 0.10 else \"✓\"\n        print(f\"Train-Val Gap: {f1_gap:+.4f} {gap_status}\")\n\n        # Save history\n        history['train_loss'].append(train_loss)\n        history['train_macro_f1'].append(train_macro_f1)\n        history['val_loss'].append(val_results['loss'])\n        history['val_macro_f1'].append(val_results['macro_f1'])\n        history['val_weighted_f1'].append(val_results['weighted_f1'])\n\n        # Save best model (based on Macro F1)\n        if val_results['macro_f1'] > best_val_macro_f1:\n            best_val_macro_f1 = val_results['macro_f1']\n            best_epoch = epoch + 1\n            patience = 0\n            \n            # Save checkpoint\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'val_macro_f1': val_results['macro_f1'],\n                'val_weighted_f1': val_results['weighted_f1'],\n                'config': CONFIG,\n                'history': history,\n                'model_name': MODEL_NAME  \n            \n            print(f\" SAVED BEST MODEL! (Val Macro-F1: {best_val_macro_f1:.4f})\")\n            \n        else:\n            patience += 1\n            print(f\"No improvement for {patience} epoch(s). Best: {best_val_macro_f1:.4f}\")\n            \n            if patience >= early_stopping_patience:\n                print(f\"\\n EARLY STOPPING TRIGGERED!\")\n                print(f\"   No improvement for {early_stopping_patience} consecutive epochs.\")\n                print(f\"   Best Val Macro F1: {best_val_macro_f1:.4f} at epoch {best_epoch}\")\n                break\n\n        # Memory monitoring (first epoch only)\n        if torch.cuda.is_available() and epoch == 0:\n            allocated = torch.cuda.memory_allocated(0) / 1024**3\n            reserved = torch.cuda.memory_reserved(0) / 1024**3\n            print(f\"GPU Memory: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n            \n    except RuntimeError as e:\n        print(f\"\\n Runtime Error: {e}\")\n        \n\nprint(\"\\n\" + \"=\"*70)\nprint(f\" TRAINING COMPLETE!\")\nprint(f\" Best Model: Epoch {best_epoch}\")\nprint(f\" Best Val Macro F1: {best_val_macro_f1:.4f}\")\nprint(\"=\"*70)\n\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Loss plot\naxes[0].plot(history['train_loss'], label='Train Loss', marker='o', linewidth=2.5, color='#3498db')\naxes[0].plot(history['val_loss'], label='Val Loss', marker='s', linewidth=2.5, color='#e74c3c')\naxes[0].axvline(x=best_epoch-1, color='green', linestyle='--', alpha=0.7, linewidth=2, label=f'Best Epoch ({best_epoch})')\naxes[0].set_xlabel('Epoch', fontsize=13, fontweight='bold')\naxes[0].set_ylabel('Loss', fontsize=13, fontweight='bold')\naxes[0].set_title('Training and Validation Loss', fontsize=15, fontweight='bold')\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3, linestyle='--')\n\n# F1 plot with targets\naxes[1].plot(history['train_macro_f1'], label='Train Macro F1', marker='o', linewidth=2.5, color='#3498db')\naxes[1].plot(history['val_macro_f1'], label='Val Macro F1', marker='s', linewidth=2.5, color='#e74c3c')\naxes[1].axhline(y=best_val_macro_f1, color='green', linestyle='--', alpha=0.7, linewidth=2, label=f'Best: {best_val_macro_f1:.4f}')\naxes[1].axhline(y=0.82, color='gold', linestyle=':', alpha=0.7, linewidth=2.5, label='Target: 82%')\naxes[1].axhline(y=0.80, color='orange', linestyle=':', alpha=0.5, linewidth=2, label='Minimum: 80%')\naxes[1].axvline(x=best_epoch-1, color='green', linestyle='--', alpha=0.7, linewidth=2)\naxes[1].set_xlabel('Epoch', fontsize=13, fontweight='bold')\naxes[1].set_ylabel('Macro F1 Score', fontsize=13, fontweight='bold')\naxes[1].set_title('Macro F1 Score Progress', fontsize=15, fontweight='bold')\naxes[1].legend(fontsize=10, loc='lower right')\naxes[1].grid(True, alpha=0.3, linestyle='--')\naxes[1].set_ylim([0.65, 0.90])\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n Training Summary:\")\nprint(f\"  Best epoch: {best_epoch}/{CONFIG['num_epochs']}\")\nprint(f\"  Best Val Macro F1: {best_val_macro_f1:.4f}\")\nif len(history['train_macro_f1']) > 0:\n    print(f\"  Final Train F1: {history['train_macro_f1'][-1]:.4f}\")\n    print(f\"  Final Val F1: {history['val_macro_f1'][-1]:.4f}\")\n    final_gap = history['train_macro_f1'][-1] - history['val_macro_f1'][-1]\n    print(f\"  Final Train-Val Gap: {final_gap:+.4f} {'(Good generalization ✓)' if abs(final_gap) < 0.08 else '(Check for overfitting )'}\")","metadata":{"trusted":true,"id":"8rkS3d3dAAZj","outputId":"4664db2d-9d6f-41df-c3b0-01d0d1a5190e"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================\n# CELL 11: Final Evaluation with TTA\n# ====================\nimport torch\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL MODEL EVALUATION\")\nprint(\"=\"*70)\n\n# Load best model\ncheckpoint = torch.load('/kaggle/input/best-model-4-large-careful/transformers/default/1/best_model_4_large_careful.pt', weights_only=False)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nprint(f\"✓ Loaded best model from epoch {checkpoint['epoch'] + 1}\")\nprint(f\"  Best validation Macro F1: {checkpoint['val_macro_f1']:.4f}\")\nprint(f\"  Best validation Weighted F1: {checkpoint['val_weighted_f1']:.4f}\\n\")\n\n# Step 1: Optimize threshold on validation set\nprint(\"Step 1: Finding optimal threshold...\")\nbest_threshold = optimize_threshold(model, val_loader, device)\n\n# Step 2: Evaluate on test set with TTA\nprint(f\"\\nStep 2: Evaluating on test set with TTA (n={CONFIG['tta_iterations']})...\")\ntest_results = evaluate(\n    model,\n    test_loader,\n    device,\n    threshold=best_threshold,\n    use_tta=True\n)\n\n# Calculate additional metrics\ntest_preds = test_results['predictions']\ntest_labels = test_results['labels']\n\nweighted_f1 = f1_score(test_labels, test_preds, average='weighted')\nprecision, recall, f1, support = precision_recall_fscore_support(\n    test_labels, test_preds, average=None\n)\n\n# Display results\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL TEST RESULTS\")\nprint(\"=\"*70)\nprint(f\" PRIMARY METRICS:\")\nprint(f\"  {'Macro F1-Score:':<30} {test_results['macro_f1']:.4f} {'✅ TARGET MET!' if test_results['macro_f1'] >= 0.80 else '⭐ (Target: 80%+)'}\")\nprint(f\"  {'Weighted F1-Score:':<30} {weighted_f1:.4f}\")\nprint(f\"  {'Optimal Threshold:':<30} {best_threshold:.3f}\")\nprint(f\"  {'TTA Iterations:':<30} {CONFIG['tta_iterations']}\")\n\nprint(f\"\\n{'='*70}\")\nprint(\"PER-CLASS PERFORMANCE:\")\nprint(\"=\"*70)\nclass_names = ['Non-Hate (Class 0)', 'Hate Speech (Class 1)']\nfor i, (name, prec, rec, f1_class, sup) in enumerate(zip(class_names, precision, recall, f1, support)):\n    print(f\"{name}:\")\n    print(f\"  F1-Score:  {f1_class:.4f}\")\n    print(f\"  Precision: {prec:.4f}\")\n    print(f\"  Recall:    {rec:.4f}\")\n    print(f\"  Support:   {int(sup)}\")\n    print()\n\nprint(\"=\"*70)\nprint(\"DETAILED CLASSIFICATION REPORT:\")\nprint(\"=\"*70)\nprint(classification_report(test_labels, test_preds, target_names=['Non-Hate', 'Hate Speech']))\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"CONFUSION MATRIX:\")\nprint(\"=\"*70)\ncm = confusion_matrix(test_labels, test_preds)\nprint(cm)\nprint(f\"\\n  True Negatives:  {cm[0,0]}\")\nprint(f\"  False Positives: {cm[0,1]}\")\nprint(f\"  False Negatives: {cm[1,0]}\")\nprint(f\"  True Positives:  {cm[1,1]}\")\noverall_acc = accuracy_score(test_labels, test_preds)\nprint(f\"\\nOverall Test Accuracy: {overall_acc:.4f}\")\n\n# Per-Language Performance\nprint(\"\\n\" + \"=\"*70)\nprint(\"PER-LANGUAGE MACRO F1 SCORES:\")\nprint(\"=\"*70)\n\ntest_preds_array = np.array(test_preds)\ntest_labels_array = np.array(test_labels)\ntest_df_reset = test_df.reset_index(drop=True)\n\nlang_results = {}\nfor lang, lang_name in [('en', 'English'), ('es', 'Spanish'), ('de', 'German')]:\n    lang_mask = test_df_reset['language'] == lang\n    lang_indices = lang_mask[lang_mask].index.tolist()\n\n    if len(lang_indices) > 0:\n        lang_preds = test_preds_array[lang_indices]\n        lang_labels = test_labels_array[lang_indices]\n\n        lang_macro_f1 = f1_score(lang_labels, lang_preds, average='macro')\n        lang_weighted_f1 = f1_score(lang_labels, lang_preds, average='weighted')\n        lang_acc = accuracy_score(lang_labels, lang_preds)\n\n        lang_results[lang_name] = {\n            'macro_f1': lang_macro_f1,\n            'weighted_f1': lang_weighted_f1,\n            'accuracy': lang_acc\n        }\n\n        print(f\"{lang_name}:\")\n        print(f\"  Macro F1:    {lang_macro_f1:.4f}\")\n        print(f\"  Weighted F1: {lang_weighted_f1:.4f}\")\n        print(f\"  Accuracy:    {lang_acc:.4f}\")\n        print(f\"  Samples:     {len(lang_preds)}\")\n        print()\n\n# Summary\nprint(\"=\"*70)\nif test_results['macro_f1'] >= 0.80:\n    print(f\" SUCCESS! MACRO F1-SCORE: {test_results['macro_f1']:.4f}\")\n    print(f\" Target of 80%+ ACHIEVED!\")\nelse:\n    print(f\" FINAL MACRO F1-SCORE: {test_results['macro_f1']:.4f}\")\n    improvement_needed = 0.80 - test_results['macro_f1']\n    print(f\"  {improvement_needed:.2%} away from 80% target\")\nprint(\"=\"*70)\n\n# Visualize confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Non-Hate', 'Hate Speech'],\n            yticklabels=['Non-Hate', 'Hate Speech'],\n            cbar_kws={'label': 'Count'})\nplt.title(f'Confusion Matrix\\nMacro F1: {test_results[\"macro_f1\"]:.4f} | TTA: {CONFIG[\"tta_iterations\"]}', \n          fontsize=14, fontweight='bold')\nplt.ylabel('True Label', fontsize=12)\nplt.xlabel('Predicted Label', fontsize=12)\nplt.tight_layout()\nplt.show()\n\n# Detailed breakdown\nprint(f\"\\n📊 PERFORMANCE BREAKDOWN:\")\nprint(f\"  Training Optimization:\")\nprint(f\"    - Layerwise Learning Rates: \")\nprint(f\"    - OneCycleLR Scheduler \")\nprint(f\"    - Data Augmentation\")\nprint(f\"    - Class-Balanced Focal Loss\")\nprint(f\"    - Multi-Sample Dropout\")\nprint(f\"  Evaluation:\")\nprint(f\"    - Test-Time Augmentation: {CONFIG['tta_iterations']}x\")\nprint(f\"    - Threshold Optimization: {best_threshold:.3f}\")\nprint(f\"\\n  Results by Language:\")\nfor lang_name, results in lang_results.items():\n    print(f\"    {lang_name}: {results['macro_f1']:.4f}\")\n\nprint(f\"\\n Best model saved as: best_xlm_roberta_macro_f1.pt\")\nprint(f\" Final Test Macro F1: {test_results['macro_f1']:.4f}\")\nprint(f\" Validation-Test Gap: {checkpoint['val_macro_f1'] - test_results['macro_f1']:.4f}\")\n\n# Store final results\nfinal_results = {\n    'test_macro_f1': test_results['macro_f1'],\n    'test_weighted_f1': weighted_f1,\n    'test_accuracy': overall_acc,\n    'val_macro_f1': checkpoint['val_macro_f1'],\n    'optimal_threshold': best_threshold,\n    'tta_iterations': CONFIG['tta_iterations'],\n    'best_epoch': checkpoint['epoch'] + 1,\n    'per_language': lang_results\n}\n\nprint(f\"\\n Evaluation complete!\")","metadata":{"trusted":true,"id":"9qq8GZNZAAZk","outputId":"538b3e61-de67-43b1-ce28-deee5f98f994"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================\n# CELL 12: Visualizations\n# ====================\n# ====================\n# Load saved training history (if running after restart)\n# ====================\nimport json\n\nif 'history' not in globals():\n    try:\n        with open('/kaggle/input/outputjson/xlm_roberta_macro_f1_optimized_results.json', 'r') as f:\n            saved = json.load(f)\n            history = saved['training_history']\n        print(\" Loaded training history from JSON\")\n    except FileNotFoundError:\n        raise ValueError(\"History is not defined and training_results.json was not found.\")\n\n# Create results directory\nos.makedirs('results', exist_ok=True)\nos.makedirs('plots', exist_ok=True)\n\n# Plot 1: Training History\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Loss plot\naxes[0].plot(history['train_loss'], label='Train Loss', marker='o', linewidth=2)\naxes[0].plot(history['val_loss'], label='Val Loss', marker='s', linewidth=2)\naxes[0].set_xlabel('Epoch', fontsize=12)\naxes[0].set_ylabel('Loss', fontsize=12)\naxes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\n\n# F1 plot instead of accuracy\naxes[1].plot(history['train_macro_f1'], label='Train Macro F1', marker='o', linewidth=2)\naxes[1].plot(history['val_macro_f1'], label='Val Macro F1', marker='s', linewidth=2)\naxes[1].set_xlabel('Epoch', fontsize=12)\naxes[1].set_ylabel('Macro F1-Score', fontsize=12)\naxes[1].set_title('Training and Validation Macro F1-Score', fontsize=14, fontweight='bold')\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\n\n\nplt.tight_layout()\nplt.savefig('plots/training_history.pdf', dpi=300, bbox_inches='tight')\nplt.savefig('plots/training_history.png', dpi=300, bbox_inches='tight')\nplt.show()\nprint(\" Training history plot saved!\")\n","metadata":{"trusted":true,"id":"NZ3wFtANAAZl","outputId":"19424187-8934-4496-b7d2-b493a6ca49da"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================\n# CELL 13: Save Results & Final Summary (FIXED)\n# ====================\n# ====================\n# LOAD BEST EPOCH & BEST VAL F1 IF NOT DEFINED\n# ====================\nimport json\n\nif 'best_epoch' not in globals() or 'best_val_macro_f1' not in globals():\n    print(\"best_epoch / best_val_macro_f1 not found in memory — loading from results JSON...\")\n\n    # Try loading from your results file\n    try:\n        with open('/kaggle/input/outputjson/xlm_roberta_macro_f1_optimized_results.json', 'r') as f:\n            saved = json.load(f)\n\n        best_epoch = saved['training_history']['best_epoch']\n        best_val_macro_f1 = saved['training_history']['best_val_macro_f1']\n\n        print(f\" Loaded best_epoch={best_epoch}, best_val_macro_f1={best_val_macro_f1:.4f}\")\n\n    except Exception as e:\n        raise ValueError(\n            \" ERROR: best_epoch is not defined and cannot be found in saved results.\\n\"\n            \"Make sure to run the training cell first!\"\n        ) from e\n\nif 'history' not in globals():\n    try:\n        with open('/kaggle/working/results/xlm_roberta_macro_f1_optimized_results.json', 'r') as f:\n            saved = json.load(f)\n            history = saved['training_history']\n        print(\" Loaded training history from JSON\")\n    except FileNotFoundError:\n        raise ValueError(\"History is not defined and training_results.json was not found.\")\n# Extract test results from Cell 11\ntest_acc = accuracy_score(test_labels, test_preds)\nmacro_f1 = test_results['macro_f1']  # ← FIX: Get from test_results dict\nweighted_f1 = test_results['weighted_f1']  # ← FIX: Get from test_results dict\n\n# Save complete results to JSON\nresults = {\n    'model': 'XLM-RoBERTa-Large-MacroF1-Optimized',\n    'task': 'Binary Polarization Detection',\n    'configuration': CONFIG,\n    'data_info': {\n        'train_samples': len(train_df),\n        'val_samples': len(val_df),\n        'test_samples': len(test_df),\n        'num_classes': num_classes,\n        'languages': ['English', 'Spanish', 'German'],\n        'augmented_samples': 584,\n        'final_balance': '1.016:1 (50.4% vs 49.6%)'\n    },\n    'training_history': {\n        'train_loss': [float(x) for x in history['train_loss']],\n        'train_macro_f1': [float(x) for x in history['train_macro_f1']],\n        'val_loss': [float(x) for x in history['val_loss']],\n        'val_macro_f1': [float(x) for x in history['val_macro_f1']],\n        'val_weighted_f1': [float(x) for x in history['val_weighted_f1']],\n        'best_epoch': int(best_epoch),\n        'best_val_macro_f1': float(best_val_macro_f1)\n    },\n    'test_results': {\n        'accuracy': float(test_acc),\n        'macro_f1': float(macro_f1),\n        'weighted_f1': float(weighted_f1),\n        'optimal_threshold': float(best_threshold),\n        'precision': float(precision_recall_fscore_support(test_labels, test_preds, average='weighted')[0]),\n        'recall': float(precision_recall_fscore_support(test_labels, test_preds, average='weighted')[1]),\n        'tta_iterations': CONFIG['tta_iterations']\n    },\n    'per_language_results': lang_results,  \n    'per_class_f1': {\n        'non_polarized': float(test_results['per_class_f1'][0]),\n        'polarized': float(test_results['per_class_f1'][1])\n    }\n}\n\n# Create results directory if it doesn't exist\nimport os\nos.makedirs('results', exist_ok=True)\nos.makedirs('plots', exist_ok=True)\n\n# Save to JSON\nwith open('results/xlm_roberta_macro_f1_optimized_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(\" Results saved to: results/xlm_roberta_macro_f1_optimized_results.json\")\n\n# Final Summary\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL SUMMARY - XLM-RoBERTa MACRO F1 OPTIMIZED\")\nprint(\"=\"*70)\nprint(f\"Task: Binary Polarization Detection\")\nprint(f\"  Class 0: Non-polarized speech\")\nprint(f\"  Class 1: Polarized speech\")\n\nprint(f\"\\nModel: XLM-RoBERTa with Macro F1 Optimizations\")\nprint(f\"  ✓ Class-balanced focal loss (α={CONFIG['focal_alpha']}, γ={CONFIG['focal_gamma']})\")\nprint(f\"  ✓ Multi-sample dropout (n={CONFIG['num_dropouts']})\")\nprint(f\"  ✓ Attention pooling\")\nprint(f\"  ✓ Mixup augmentation (α={CONFIG['mixup_alpha']})\")\nprint(f\"  ✓ Test-time augmentation (n={CONFIG['tta_iterations']})\")\nprint(f\"  ✓ Threshold optimization\")\n\nprint(f\"\\nData:\")\nprint(f\"  Languages: English, Spanish, German\")\nprint(f\"  Training samples: {len(train_df):,} (including {results['data_info']['augmented_samples']} augmented)\")\nprint(f\"  Validation samples: {len(val_df):,}\")\nprint(f\"  Test samples: {len(test_df):,}\")\nprint(f\"  Balance: {results['data_info']['final_balance']}\")\n\nprint(f\"\\nTraining:\")\nprint(f\"  Best Epoch: {best_epoch}\")\nprint(f\"  Best Val Macro F1: {best_val_macro_f1:.4f}\")\nprint(f\"  Training time: ~25 minutes\")\nprint(f\"  Early stopping: Triggered at epoch 9 (patience=3)\")\n\nprint(f\"\\n\" + \"=\"*70)\nprint(f\"TEST PERFORMANCE (PRIMARY RESULTS)\")\nprint(f\"=\"*70)\nprint(f\"   Macro F1:    {macro_f1:.4f} (Target: 80%)\")\nprint(f\"   Weighted F1: {weighted_f1:.4f}\")\nprint(f\"   Accuracy:    {test_acc:.4f}\")\nprint(f\"    Threshold:   {best_threshold:.3f}\")\n\nprint(f\"\\nPer-Class Performance:\")\nprint(f\"  Non-Polarized (Class 0): F1 = {test_results['per_class_f1'][0]:.4f}\")\nprint(f\"  Polarized (Class 1):     F1 = {test_results['per_class_f1'][1]:.4f}\")\n\nprint(f\"\\nPer-Language Macro F1:\")\nfor lang, metrics in lang_results.items():\n    print(f\"  {lang:8s}: {metrics['macro_f1']:.4f}\")\n\nprint(\"\\n\" + \"=\"*70)\n\nprint(f\"  Current Test F1: {macro_f1:.4f}\")\n\n\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"FILES SAVED\")\nprint(\"=\"*70)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\" EVALUATION COMPLETE!\")\nprint(\"=\"*70)\n\nprint(\"\\n Creating summary visualizations...\")\n\nimport matplotlib.pyplot as plt\n\n# Create a summary figure\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Plot 1: Per-Language F1\nax1 = axes[0, 0]\nlanguages = list(lang_results.keys())\nf1_scores = [m['macro_f1'] for m in lang_results.values()]\n\ncolors = ['#3498db' if f1 < 0.75 else '#2ecc71' if f1 < 0.80 else '#27ae60' for f1 in f1_scores]\n\nbars = ax1.bar(languages, f1_scores, color=colors, alpha=0.7, edgecolor='black')\nax1.axhline(y=0.80, color='red', linestyle='--', linewidth=2, label='Target (80%)', alpha=0.7)\nax1.axhline(y=macro_f1, color='orange', linestyle=':', linewidth=2, label=f'Overall ({macro_f1:.2%})', alpha=0.7)\nax1.set_ylabel('Macro F1 Score', fontsize=12, fontweight='bold')\nax1.set_title('Per-Language Performance', fontsize=14, fontweight='bold')\nax1.legend(fontsize=10)\nax1.grid(axis='y', alpha=0.3)\n\n# Add value labels on bars\nfor i, (lang, f1) in enumerate(zip(languages, f1_scores)):\n    ax1.text(i, f1 + 0.01, f'{f1:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n\n# Plot 2: Confusion Matrix\nax2 = axes[0, 1]\ncm = confusion_matrix(test_labels, test_preds)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2, cbar_kws={'label': 'Count'})\nax2.set_xlabel('Predicted', fontsize=12, fontweight='bold')\nax2.set_ylabel('Actual', fontsize=12, fontweight='bold')\nax2.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\nax2.set_xticklabels(['Non-Polarized', 'Polarized'])\nax2.set_yticklabels(['Non-Polarized', 'Polarized'])\n\n# Plot 3: Training History\nax3 = axes[1, 0]\nepochs = range(1, len(history['val_macro_f1']) + 1)\nax3.plot(epochs, history['train_macro_f1'], 'o-', label='Train F1', linewidth=2, markersize=6)\nax3.plot(epochs, history['val_macro_f1'], 's-', label='Val F1', linewidth=2, markersize=6)\nax3.axvline(x=best_epoch, color='green', linestyle='--', linewidth=2, alpha=0.5, label=f'Best Epoch ({best_epoch})')\nax3.axhline(y=0.80, color='red', linestyle=':', linewidth=2, alpha=0.5, label='Target (80%)')\nax3.set_xlabel('Epoch', fontsize=12, fontweight='bold')\nax3.set_ylabel('Macro F1 Score', fontsize=12, fontweight='bold')\nax3.set_title('Training Progress', fontsize=14, fontweight='bold')\nax3.legend(fontsize=10)\nax3.grid(True, alpha=0.3)\n\n# Plot 4: Performance Metrics Summary\nax4 = axes[1, 1]\nmetrics = ['Macro F1', 'Weighted F1', 'Accuracy']\nscores = [macro_f1, weighted_f1, test_acc]\ncolors_metrics = ['#e74c3c' if s < 0.75 else '#f39c12' if s < 0.80 else '#2ecc71' for s in scores]\n\nbars = ax4.barh(metrics, scores, color=colors_metrics, alpha=0.7, edgecolor='black')\nax4.axvline(x=0.80, color='red', linestyle='--', linewidth=2, label='Target', alpha=0.7)\nax4.set_xlabel('Score', fontsize=12, fontweight='bold')\nax4.set_title('Overall Test Metrics', fontsize=14, fontweight='bold')\nax4.set_xlim(0, 1)\nax4.legend(fontsize=10)\nax4.grid(axis='x', alpha=0.3)\n\n# Add value labels\nfor i, (metric, score) in enumerate(zip(metrics, scores)):\n    ax4.text(score + 0.01, i, f'{score:.3f}', va='center', fontsize=11, fontweight='bold')\n\nplt.suptitle(f'XLM-RoBERTa Polarization Detection - Final Results\\nTest Macro F1: {macro_f1:.4f}', \n             fontsize=16, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.savefig('plots/final_summary.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\" Summary visualization saved to: plots/final_summary.png\")\nprint(\"\\n All done! Check the results folder for detailed outputs.\")","metadata":{"trusted":true,"id":"hGXSndLsAAZm","outputId":"52f54376-12d8-41fa-ef6c-122075c9ef27"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ====================\n# CELL 12: Predict on Unlabeled Dev Set\n# ====================\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\nprint(\"=\"*70)\nprint(\"PREDICTING ON UNLABELED DEV SET\")\nprint(\"=\"*70)\n# Fallback config (in case not saved in checkpoint)\nDEFAULT_CONFIG = {\n    'model_name': 'xlm-roberta-large',\n    'dropout': 0.2,\n    'num_dropouts': 5,\n    'focal_alpha': 0.25,\n    'focal_gamma': 2.0,\n    'batch_size': 8,\n    'tta_iterations': 5\n}\n# ============================================\n# STEP 1: Load Dev Set\n# ============================================\ndef load_dev_data():\n    \"\"\"Load all three dev sets (English, Spanish, German).\"\"\"\n    print(\"\\nLoading dev set files...\")\n    \n    # Load dev data\n    eng_dev = pd.read_csv('/kaggle/input/dev-dataset/eng_dev.csv', encoding='utf-8')\n    spa_dev = pd.read_csv('/kaggle/input/dev-dataset/spa_dev.csv', encoding='utf-8')\n    deu_dev = pd.read_csv('/kaggle/input/dev-dataset/deu_dev.csv', encoding='utf-8')\n    \n    # Add language identifier\n    eng_dev['language'] = 'en'\n    spa_dev['language'] = 'es'\n    deu_dev['language'] = 'de'\n    \n    # Combine all dev data\n    dev_df = pd.concat([eng_dev, spa_dev, deu_dev], ignore_index=True)\n    \n    print(f\"✓ Dev data loaded!\")\n    print(f\"  English:  {len(eng_dev)}\")\n    print(f\"  Spanish:  {len(spa_dev)}\")\n    print(f\"  German:   {len(deu_dev)}\")\n    print(f\"  Total:    {len(dev_df)}\")\n    \n    return dev_df, eng_dev, spa_dev, deu_dev\n\n# ============================================\n# STEP 2: Clean Dev Data\n# ============================================\ndef clean_dev_data(df):\n    \"\"\"Minimal cleaning - keep duplicates!\"\"\"\n    print(\"\\nCleaning dev data...\")\n    initial_size = len(df)\n    \n    df = df.dropna(subset=['text'])\n    df = df[df['text'].str.strip() != '']\n    df = df.reset_index(drop=True)\n    \n    print(f\"  {initial_size} → {len(df)} samples\")\n    return df\n\n# ============================================\n# STEP 3: Dataset for Prediction\n# ============================================\nclass DevDataset(torch.utils.data.Dataset):\n    def __init__(self, texts, tokenizer, max_length=128):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten()\n        }\n\n# ============================================\n# STEP 4: Prediction with TTA\n# ============================================\ndef predict_with_tta_batch(model, batch, device, threshold=0.5, n_iterations=5):\n    model.eval()\n    all_logits = []\n    \n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    \n    with torch.no_grad():\n        for _ in range(n_iterations):\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            all_logits.append(outputs['logits'])\n    \n    avg_logits = torch.stack(all_logits).mean(dim=0)\n    probs = F.softmax(avg_logits, dim=1)\n    preds = (probs[:, 1] >= threshold).long()\n    \n    return preds.cpu().numpy(), probs.cpu().numpy()\n\n# ============================================\n# STEP 5: Generate Predictions\n# ============================================\ndef generate_predictions(model, dev_df, tokenizer, device, threshold=0.5, batch_size=8, use_tta=True, tta_iterations=5):\n    print(f\"\\nGenerating predictions...\")\n    print(f\"  TTA: {use_tta} ({tta_iterations} iterations)\" if use_tta else \"  TTA: False\")\n    print(f\"  Threshold: {threshold:.3f}\")\n    print(f\"  Batch size: {batch_size}\")\n    \n    dev_dataset = DevDataset(dev_df['text'].tolist(), tokenizer, max_length=128)\n    dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n    \n    all_preds = []\n    all_probs = []\n    model.eval()\n    \n    for batch in tqdm(dev_loader, desc='Predicting'):\n        if use_tta:\n            preds, probs = predict_with_tta_batch(model, batch, device, threshold, tta_iterations)\n        else:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            with torch.no_grad():\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n                probs = F.softmax(outputs['logits'], dim=1)\n                preds = (probs[:, 1] >= threshold).long()\n            \n            preds = preds.cpu().numpy()\n            probs = probs.cpu().numpy()\n        \n        all_preds.extend(preds)\n        all_probs.extend(probs)\n    \n    return np.array(all_preds), np.array(all_probs)\n\n# ============================================\n# MAIN EXECUTION\n# ============================================\n\n# Load dev data\ndev_df, eng_dev, spa_dev, deu_dev = load_dev_data()\n\n# Clean\ndev_df_clean = clean_dev_data(dev_df)\n\n# Load model\nprint(\"\\n\" + \"=\"*70)\nprint(\"Loading best model...\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ncheckpoint = torch.load('/kaggle/input/best-model-4-large-careful/transformers/default/1/best_model_4_large_careful.pt', weights_only=False)\n\n# Get saved config\nsaved_config = checkpoint.get('config', DEFAULT_CONFIG)\n\n# Initialize model\nclass_weights = torch.tensor([1.0, 1.0], dtype=torch.float32).to(device)\nmodel = MacroF1OptimizedModel(\n    num_classes=2,\n    model_name=saved_config['model_name'],\n    dropout=saved_config['dropout'],\n    num_dropouts=saved_config['num_dropouts'],\n    class_weights=class_weights,\n    focal_alpha=saved_config['focal_alpha'],\n    focal_gamma=saved_config['focal_gamma']\n)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel = model.to(device)\nmodel.eval()\n\nprint(f\"✓ Model loaded! Val Macro F1: {checkpoint['val_macro_f1']:.4f}\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(saved_config['model_name'])\nprint(\"✓ Tokenizer loaded!\")\n\n# Get optimal threshold (or use default)\ntry:\n    optimal_threshold = best_threshold\n    print(f\"\\n✓ Using optimized threshold: {optimal_threshold:.3f}\")\nexcept NameError:\n    optimal_threshold = 0.5\n    print(f\"\\n⚠ Using default threshold: {optimal_threshold:.3f}\")\n\n# Generate predictions\npredictions, probabilities = generate_predictions(\n    model=model,\n    dev_df=dev_df_clean,\n    tokenizer=tokenizer,\n    device=device,\n    threshold=optimal_threshold,\n    batch_size=saved_config['batch_size'],\n    use_tta=True,\n    tta_iterations=saved_config.get('tta_iterations', 5)\n)\n\n# Add to dataframe\ndev_df_clean['polarization'] = predictions\ndev_df_clean['confidence'] = probabilities[:, 1]\n\n# Statistics\nprint(\"\\n\" + \"=\"*70)\nprint(\"PREDICTION STATISTICS\")\nprint(\"=\"*70)\nprint(f\"Total: {len(predictions)}\")\nprint(f\"  Class 0 (Non-polarized): {(predictions == 0).sum()} ({(predictions == 0).sum()/len(predictions)*100:.1f}%)\")\nprint(f\"  Class 1 (Polarized):     {(predictions == 1).sum()} ({(predictions == 1).sum()/len(predictions)*100:.1f}%)\")\n\nprint(f\"\\nPer-language:\")\nfor lang, lang_name in [('en', 'English'), ('es', 'Spanish'), ('de', 'German')]:\n    lang_mask = dev_df_clean['language'] == lang\n    lang_preds = predictions[lang_mask]\n    polarized = (lang_preds == 1).sum()\n    print(f\"  {lang_name}: {len(lang_preds)} total, {polarized} polarized ({polarized/len(lang_preds)*100:.1f}%)\")\n\n# ============================================\n# SAVE PREDICTIONS\n# ============================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"SAVING PREDICTIONS\")\nprint(\"=\"*70)\n\n# Combined file\ndev_df_clean[['id', 'text', 'polarization', 'language']].to_csv(\n    '/kaggle/working/dev_predictions_all.csv', index=False, encoding='utf-8'\n)\nprint(\"✓ Saved: /kaggle/working/dev_predictions_all.csv\")\n\n# Individual language files\nfor lang, original_df, lang_name in [\n    ('en', eng_dev, 'English'),\n    ('es', spa_dev, 'Spanish'),\n    ('de', deu_dev, 'German')\n]:\n    lang_predictions = dev_df_clean[dev_df_clean['language'] == lang].copy()\n    \n    # Drop the empty polarization column from original if it exists\n    if 'polarization' in original_df.columns:\n        original_df = original_df.drop(columns=['polarization'])\n    \n    # Merge predictions\n    original_df_with_preds = original_df.merge(\n        lang_predictions[['id', 'polarization']],\n        on='id',\n        how='left'\n    )\n    \n    # Remove language column if you don't want it\n    if 'language' in original_df_with_preds.columns:\n        original_df_with_preds = original_df_with_preds.drop(columns=['language'])\n    \n    # Save with only id, text, polarization\n    original_df_with_preds.to_csv(\n        f'/kaggle/working/{lang}_dev_predictions.csv',\n        index=False,\n        encoding='utf-8'\n    )\n    print(f\" Saved: /kaggle/working/{lang}_dev_predictions.csv\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"COMPLETE!\")\nprint(\"=\"*70)\nprint(\"\\nOutput files:\")\nprint(\"  1. dev_predictions_all.csv (combined)\")\nprint(\"  2. en_dev_predictions.csv\")\nprint(\"  3. es_dev_predictions.csv\")\nprint(\"  4. de_dev_predictions.csv\")\nprint(\"\\nColumns: id, text, polarization (0/1), confidence, language\")\nprint(\"=\"*70)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T13:51:19.937902Z","iopub.execute_input":"2025-11-17T13:51:19.938709Z","iopub.status.idle":"2025-11-17T13:52:23.677246Z","shell.execute_reply.started":"2025-11-17T13:51:19.938673Z","shell.execute_reply":"2025-11-17T13:52:23.676339Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nPREDICTING ON UNLABELED DEV SET\n======================================================================\n\nLoading dev set files...\n✓ Dev data loaded!\n  English:  133\n  Spanish:  165\n  German:   159\n  Total:    457\n\nCleaning dev data...\n  457 → 457 samples\n\n======================================================================\nLoading best model...\n✓ Model loaded! Val Macro F1: 0.7585\n✓ Tokenizer loaded!\n\n⚠ Using default threshold: 0.500\n\nGenerating predictions...\n  TTA: True (5 iterations)\n  Threshold: 0.500\n  Batch size: 8\n","output_type":"stream"},{"name":"stderr","text":"Predicting: 100%|██████████| 58/58 [00:52<00:00,  1.11it/s]","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nPREDICTION STATISTICS\n======================================================================\nTotal: 457\n  Class 0 (Non-polarized): 208 (45.5%)\n  Class 1 (Polarized):     249 (54.5%)\n\nPer-language:\n  English: 133 total, 66 polarized (49.6%)\n  Spanish: 165 total, 88 polarized (53.3%)\n  German: 159 total, 95 polarized (59.7%)\n\n======================================================================\nSAVING PREDICTIONS\n======================================================================\n✓ Saved: /kaggle/working/dev_predictions_all.csv\n Saved: /kaggle/working/en_dev_predictions.csv\n Saved: /kaggle/working/es_dev_predictions.csv\n Saved: /kaggle/working/de_dev_predictions.csv\n\n======================================================================\nCOMPLETE!\n======================================================================\n\nOutput files:\n  1. dev_predictions_all.csv (combined)\n  2. en_dev_predictions.csv\n  3. es_dev_predictions.csv\n  4. de_dev_predictions.csv\n\nColumns: id, text, polarization (0/1), confidence, language\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5}]}