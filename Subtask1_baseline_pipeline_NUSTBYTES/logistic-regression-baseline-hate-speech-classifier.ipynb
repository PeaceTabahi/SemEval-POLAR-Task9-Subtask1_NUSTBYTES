{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TaBVS89yP2s",
        "outputId": "0823fcef-1bf6-4ff4-dea8-1a5eb18d3107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.13.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (1.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.6.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (1.0.9)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (0.19.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (5.2.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (2.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->-r requirements.txt (line 6)) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->-r requirements.txt (line 6)) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->-r requirements.txt (line 6)) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->-r requirements.txt (line 6)) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.16.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect->-r requirements.txt (line 8)) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chardet\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import emoji\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import unicodedata\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "from nltk.util import ngrams\n",
        "\n",
        "nltk.download('stopwords')\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('punkt', quiet=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApKXUJz4y8K2",
        "outputId": "05390ac0-1a8e-4616-c2ca-56d7fee2beee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size = 5><b><u>Preprocessing</u></b></font>"
      ],
      "metadata": {
        "id": "kkUOTV-91v-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def clean_text(s):\n",
        "    \"\"\"Clean a single text entry.\"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    # fix encoding weirdness and accents\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    # lowercase\n",
        "    s = s.lower()\n",
        "    # remove URLs\n",
        "    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)\n",
        "    # remove mentions and hashtags\n",
        "    s = re.sub(r\"@\\w+|#\\w+\", \" \", s)\n",
        "    # keep letters (English + Spanish + German)\n",
        "    s = re.sub(r\"[^a-zA-ZáéíóúñüäößÄÖÜ ]\", \" \", s)\n",
        "    # collapse multiple spaces\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    s = emoji.replace_emoji(s, replace='')\n",
        "    return s\n",
        "\n",
        "def preprocess_file(input_path, output_path):\n",
        "    \"\"\"Load, clean, and save CSV.\"\"\"\n",
        "    print(f\"\\nProcessing {input_path}...\")\n",
        "    df = pd.read_csv(input_path, encoding=\"utf-8\")\n",
        "\n",
        "    # detect text column\n",
        "    possible = [c for c in df.columns if c.lower() in (\"text\")]\n",
        "    text_col = possible[0] if possible else df.columns[0]\n",
        "\n",
        "    # keep text + label if available\n",
        "    cols = ['id',text_col]\n",
        "    if 'polarization' in df.columns:\n",
        "        cols.append('polarization')\n",
        "    df = df[[c for c in cols if c in df.columns]]\n",
        "\n",
        "    # clean text\n",
        "    df['text'] = df[text_col].apply(clean_text)\n",
        "\n",
        "    # drop blanks + duplicates\n",
        "    df = df[df['text'].str.strip() != \"\"]\n",
        "    df = df.drop_duplicates(subset=['text'])\n",
        "\n",
        "    # save cleaned file\n",
        "    df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"Saved cleaned file: {output_path} ({len(df)} rows)\")\n",
        "\n",
        "# run for all three\n",
        "files = [\n",
        "    (\"data/eng.csv\", \"data/eng_clean.csv\"),\n",
        "    (\"data/deu.csv\", \"data/deu_clean.csv\"),\n",
        "    (\"data/spa.csv\", \"data/spa_clean.csv\")\n",
        "]\n",
        "\n",
        "for inp, out in files:\n",
        "    preprocess_file(inp, out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUuhnRL7ze1b",
        "outputId": "5ec9fc6f-cdc3-4174-e686-0087a3111c4f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing data/eng.csv...\n",
            "Saved cleaned file: data/eng_clean.csv (2671 rows)\n",
            "\n",
            "Processing data/deu.csv...\n",
            "Saved cleaned file: data/deu_clean.csv (3179 rows)\n",
            "\n",
            "Processing data/spa.csv...\n",
            "Saved cleaned file: data/spa_clean.csv (3302 rows)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size = 5>data_loader.py</font>"
      ],
      "metadata": {
        "id": "-52AnIhF2po9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_eng_train = pd.read_csv(\"data/eng_clean.csv\", encoding=\"utf-8-sig\")\n",
        "df_eng_train = df_eng_train[['text', 'polarization']]\n",
        "df_eng_test= pd.read_csv(\"data/eng_test.csv\", encoding=\"utf-8-sig\")\n",
        "df_eng_test = df_eng_test[['text', 'polarization']]\n",
        "\n",
        "df_deu_train = pd.read_csv(\"data/deu_clean.csv\", encoding=\"utf-8-sig\")\n",
        "df_deu_train = df_deu_train[['text', 'polarization']]\n",
        "df_deu_test= pd.read_csv(\"data/deu_test.csv\", encoding=\"utf-8-sig\")\n",
        "df_deu_test = df_deu_test[['text', 'polarization']]\n",
        "\n",
        "df_spa_train = pd.read_csv(\"data/spa_clean.csv\", encoding=\"utf-8-sig\")\n",
        "df_spa_train = df_spa_train[['text', 'polarization']]\n",
        "df_spa_test= pd.read_csv(\"data/spa_test.csv\", encoding=\"utf-8-sig\")\n",
        "df_spa_test = df_spa_test[['text', 'polarization']]\n",
        "print(df_eng_train.head())\n",
        "print(df_deu_train.head())\n",
        "print(df_spa_train.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jqnM3cJ2g9M",
        "outputId": "376698ab-f04c-45d8-96aa-3465780cc6b5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  polarization\n",
            "0           is defending imperialism in the dnd chat             0\n",
            "1  still playing with this i am now following rac...             0\n",
            "2  senate gov theres groups out there republicans...             0\n",
            "3  abc md david anderson said the additional fund...             0\n",
            "4  bad people i have some conservative values so ...             0\n",
            "                                                text  polarization\n",
            "0    natürlich bin ich linksgrün ich habe herz u n d             0\n",
            "1  schuld sind habeck die grünen und diese linksg...             1\n",
            "2  vielleicht ist da ja tatsächlich was dran höch...             1\n",
            "3  so noch schnell alle linksgrün versifften demo...             1\n",
            "4  ich drücke der störchin die daumen berlin ist ...             1\n",
            "                                                text  polarization\n",
            "0  bueno tirando y si hay repregunta entonces pal...             0\n",
            "1                  caimos en su retorica de indigena             0\n",
            "2       cara de indigena sudaca porque pio asi luego             0\n",
            "3  violar a una nina es menos grave que un aborto...             1\n",
            "4                yo decido quien es judio y quien no             1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ====== Load your cleaned data ======\n",
        "df_eng = pd.read_csv(\"data/eng_clean.csv\", encoding=\"utf-8-sig\")[['text', 'polarization']]\n",
        "df_deu = pd.read_csv(\"data/deu_clean.csv\", encoding=\"utf-8-sig\")[['text', 'polarization']]\n",
        "df_spa = pd.read_csv(\"data/spa_clean.csv\", encoding=\"utf-8-sig\")[['text', 'polarization']]\n",
        "\n",
        "# ====== Split into train/test ======\n",
        "df_eng_train, df_eng_test = train_test_split(df_eng, test_size=0.2, random_state=42, stratify=df_eng['polarization'])\n",
        "df_deu_train, df_deu_test = train_test_split(df_deu, test_size=0.2, random_state=42, stratify=df_deu['polarization'])\n",
        "df_spa_train, df_spa_test = train_test_split(df_spa, test_size=0.2, random_state=42, stratify=df_spa['polarization'])\n",
        "\n",
        "# Optional: save these splits for future use\n",
        "df_eng_train.to_csv(\"data/eng_train.csv\", index=False)\n",
        "df_eng_test.to_csv(\"data/eng_test.csv\", index=False)\n",
        "df_deu_train.to_csv(\"data/deu_train.csv\", index=False)\n",
        "df_deu_test.to_csv(\"data/deu_test.csv\", index=False)\n",
        "df_spa_train.to_csv(\"data/spa_train.csv\", index=False)\n",
        "df_spa_test.to_csv(\"data/spa_test.csv\", index=False)\n",
        "\n",
        "# ====== Function to generate vocab ======\n",
        "def get_combined_vocab(texts, lang='english'):\n",
        "    stop_words = set(stopwords.words(lang))\n",
        "    if lang != 'english':\n",
        "        stop_words.update(stopwords.words('english'))\n",
        "\n",
        "    tokens_list = []\n",
        "    for txt in texts:\n",
        "        tokens = re.findall(r'\\b\\w+\\b', str(txt).lower())\n",
        "        tokens = [t for t in tokens if t not in stop_words and len(t) > 2 and t.isalpha()]\n",
        "        tokens_list.extend(tokens)\n",
        "\n",
        "    vocab_set = set()\n",
        "    # unigrams\n",
        "    for token in tokens_list:\n",
        "        vocab_set.add(token)\n",
        "    # bigrams and trigrams\n",
        "    for n in [2, 3]:\n",
        "        for gram in ngrams(tokens_list, n):\n",
        "            if len(set(gram)) > 1:\n",
        "                vocab_set.add(' '.join(gram))\n",
        "    return list(vocab_set)\n",
        "\n",
        "# ====== Generate vocab per class and save ======\n",
        "datasets = {\n",
        "    \"english\": df_eng_train,\n",
        "    \"german\": df_deu_train,\n",
        "    \"spanish\": df_spa_train\n",
        "}\n",
        "\n",
        "for lang, df in datasets.items():\n",
        "    vocab_dict = {}\n",
        "    for label, label_name in [(0, 'nonpolarized'), (1, 'polarized')]:\n",
        "        texts = df[df['polarization'] == label]['text']\n",
        "        vocab_dict[label_name] = get_combined_vocab(texts, lang=lang)\n",
        "\n",
        "    # save vocab for this language\n",
        "    with open(f\"{lang}_vocab.pkl\", \"wb\") as f:\n",
        "        pickle.dump(vocab_dict, f)\n",
        "\n",
        "    print(f\"✅ Saved {lang.capitalize()} vocabulary (unigrams+bigrams+trigrams) by class as {lang}_vocab.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw43HXWd_lK8",
        "outputId": "a425c646-6f91-401f-8be9-a89905523f0a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved English vocabulary (unigrams+bigrams+trigrams) by class as english_vocab.pkl\n",
            "✅ Saved German vocabulary (unigrams+bigrams+trigrams) by class as german_vocab.pkl\n",
            "✅ Saved Spanish vocabulary (unigrams+bigrams+trigrams) by class as spanish_vocab.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"english_vocab.pkl\", \"rb\") as f:\n",
        "    english_vocab = pickle.load(f)\n",
        "\n",
        "print(english_vocab['polarized'][:10])  # first 10 ngrams for polarized class\n",
        "print(english_vocab['nonpolarized'][:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJhweQNZDt8S",
        "outputId": "e6a1dc97-5c7b-471b-9983-60092b0974e7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['breathes fox', 'essence trumpism', 'gaza israel woke', 'crisis', 'thwarting people', 'thats right', 'canadian literature cant', 'fever', 'cold', 'states screwed emboldened']\n",
            "['think middle east', 'gallego announce', 'crisis', 'saw mike', 'putins relationship', 'husband goes', 'even bill clinton', 'ceasefire deal', 'dunno tell', 'immigration aiming']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# ====== Load vocabularies ======\n",
        "with open(\"english_vocab.pkl\", \"rb\") as f:\n",
        "    eng_vocab = pickle.load(f)\n",
        "with open(\"german_vocab.pkl\", \"rb\") as f:\n",
        "    deu_vocab = pickle.load(f)\n",
        "with open(\"spanish_vocab.pkl\", \"rb\") as f:\n",
        "    spa_vocab = pickle.load(f)\n",
        "eng_vocab = list(set(eng_vocab['polarized'] + eng_vocab['nonpolarized']))\n",
        "deu_vocab =list(set( deu_vocab['polarized'] + deu_vocab['nonpolarized']))\n",
        "spa_vocab =list(set( spa_vocab['polarized'] + spa_vocab['nonpolarized']))\n",
        "# ====== Function to train and evaluate TF-IDF + Logistic Regression ======\n",
        "def train_eval(df_train, df_test, vocab, lang_name):\n",
        "    print(f\"\\n===== {lang_name.upper()} =====\")\n",
        "\n",
        "    # TF-IDF vectorizer using your saved vocabulary\n",
        "    tfidf = TfidfVectorizer(vocabulary=vocab)\n",
        "\n",
        "    # Fit & transform train set\n",
        "    X_train = tfidf.fit_transform(df_train['text'])\n",
        "    y_train = df_train['polarization']\n",
        "\n",
        "    # Transform test set\n",
        "    X_test = tfidf.transform(df_test['text'])\n",
        "    y_test = df_test['polarization']\n",
        "\n",
        "    # Logistic Regression classifier\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "    return tfidf, clf\n",
        "\n",
        "# ====== Train & evaluate for each language ======\n",
        "tfidf_eng, clf_eng = train_eval(df_eng_train, df_eng_test, eng_vocab, \"English\")\n",
        "tfidf_deu, clf_deu = train_eval(df_deu_train, df_deu_test, deu_vocab, \"German\")\n",
        "tfidf_spa, clf_spa = train_eval(df_spa_train, df_spa_test, spa_vocab, \"Spanish\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qnhk3ZZ854XF",
        "outputId": "1fe22e4d-7c5b-4a54-bfa0-8c1d0ab99b5b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== ENGLISH =====\n",
            "Accuracy: 0.7102803738317757\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.88      0.79       334\n",
            "           1       0.69      0.42      0.52       201\n",
            "\n",
            "    accuracy                           0.71       535\n",
            "   macro avg       0.70      0.65      0.66       535\n",
            "weighted avg       0.71      0.71      0.69       535\n",
            "\n",
            "\n",
            "===== GERMAN =====\n",
            "Accuracy: 0.6729559748427673\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.70      0.69       334\n",
            "           1       0.66      0.64      0.65       302\n",
            "\n",
            "    accuracy                           0.67       636\n",
            "   macro avg       0.67      0.67      0.67       636\n",
            "weighted avg       0.67      0.67      0.67       636\n",
            "\n",
            "\n",
            "===== SPANISH =====\n",
            "Accuracy: 0.6656580937972768\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.71      0.68       329\n",
            "           1       0.68      0.62      0.65       332\n",
            "\n",
            "    accuracy                           0.67       661\n",
            "   macro avg       0.67      0.67      0.67       661\n",
            "weighted avg       0.67      0.67      0.67       661\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RJ1aifUSzoo4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}